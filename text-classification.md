### FastText
- Efficient Estimation of Word Representations in Vector Space  
https://arxiv.org/abs/1301.3781
  
- Bag of Tricks for Efficient Text Classification  
https://arxiv.org/abs/1607.01759  

### CNN
- Convolutional Neural Networks for Sentence Classification  
https://arxiv.org/abs/1408.5882  
  
- A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification  
https://arxiv.org/abs/1510.03820  
   
- Character-level Convolutional Networks for Text Classification  
https://arxiv.org/abs/1509.01626 

### RNN  
- Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks  
https://arxiv.org/abs/1503.00075  
  
- Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings  
https://arxiv.org/abs/1602.02373  
  
- Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling  
https://arxiv.org/abs/1611.06639  
  

### RCNN
- Recurrent Convolutional Neural Networks for Text Classification  
https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745  
  
### DPCNN
- Deep Pyramid Convolutional Neural Networks for Text Categorization  
https://www.aclweb.org/anthology/P17-1052/  
  
### HAN  
- Hierarchical Attention Networks for Document Classification  
https://www.aclweb.org/anthology/N16-1174/  

### Capsule
- Dynamic Routing Between Capsules  
https://arxiv.org/abs/1710.09829  
  
- Matrix Capsules with EM Routing   
https://openreview.net/pdf?id=HJWLfGWRb  
  
- Investigating Capsule Networks with Dynamic Routing for Text Classification  
https://arxiv.org/abs/1804.00538  

### GCN  
- Graph Convolutional Networks for Text Classification  
https://arxiv.org/abs/1809.05679  
  
### Transformer
#### seq2seq   
- Sequence to Sequence Learning with Neural Networks  
https://arxiv.org/abs/1409.3215  
  
- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation  
https://arxiv.org/abs/1406.1078  

#### attention  
- Neural Machine Translation by Jointly Learning to Align and Translate  
https://arxiv.org/abs/1409.0473  
  
- Effective Approaches to Attention-based Neural Machine Translation  
https://arxiv.org/abs/1508.04025v3  

#### transformer
- Attention Is All You Need  
https://arxiv.org/abs/1706.03762  

### Language Model
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  
https://arxiv.org/abs/1810.04805  
  
  (Please see more details in pretrained language model)
