### ELMO  
- Deep contextualized word representations  
https://arxiv.org/abs/1802.05365  

### Ulmfit
- Universal Language Model Fine-tuning for Text Classification  
https://arxiv.org/abs/1801.06146

### Bert
#### seq2seq 
 
- Sequence to Sequence Learning with Neural Networks  
https://arxiv.org/abs/1409.3215  
  
- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation  
https://arxiv.org/abs/1406.1078  

#### attention

- Neural Machine Translation by Jointly Learning to Align and Translate  
https://arxiv.org/abs/1409.0473  
  
- Effective Approaches to Attention-based Neural Machine Translation  
https://arxiv.org/abs/1508.04025v3  

#### transformer

- Attention Is All You Need  
https://arxiv.org/abs/1706.03762  

- Synthesizer: Rethinking Self-Attention in Transformer Models  
https://arxiv.org/abs/2005.00743  
  
#### bert
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  
https://arxiv.org/abs/1810.04805  

### GPT
- Improving Language Understanding by Generative Pre-Training  
https://openai.com/blog/language-unsupervised/  

### GPT-2
- Language Models are Unsupervised Multitask Learners  
https://openai.com/blog/better-language-models/  

### RoBERTa
- RoBERTa: A Robustly Optimized BERT Pretraining Approach  
https://arxiv.org/abs/1907.11692  

### XLNet
- Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context  
https://arxiv.org/abs/1901.02860  
  
- XLNet: Generalized Autoregressive Pretraining for Language Understanding  
https://arxiv.org/abs/1906.08237  

### SpanBERT
- SpanBERT: Improving Pre-training by Representing and Predicting Spans  
https://arxiv.org/abs/1907.10529  

### TinyBERT
- TinyBERT: Distilling BERT for Natural Language Understanding  
https://arxiv.org/abs/1909.10351  

### ALBERT
- ALBERT: A Lite BERT for Self-supervised Learning of Language Representations  
https://arxiv.org/abs/1909.11942?context=cs

### DistilBERT
- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter  
https://arxiv.org/abs/1910.01108

### T5    
- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer  
https://arxiv.org/abs/1910.10683  

### ZEN
- ZEN: A BERT-based Chinese Text Encoder Enhanced by N-gram Representations  
https://arxiv.org/abs/1911.00720  

### ELECTRA
- ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators  
https://openreview.net/forum?id=r1xMH1BtvB  
